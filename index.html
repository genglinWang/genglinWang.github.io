<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Genglin Wang</title>

    <meta name="author" content="Genglin Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jon Barron
                </p>
                <p>I'm a research scientist at <a href="https://ai.google/research">Google Research</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>.
                </p>
                <p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp;
				  <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp;
				  <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jonbarron/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JonBarron.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/smerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smerf.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
     
            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/refnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/refnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function refnerf_start() {
                    document.getElementById('refnerf_image').style.opacity = "1";
                  }

                  function refnerf_stop() {
                    document.getElementById('refnerf_image').style.opacity = "0";
                  }
                  refnerf_stop()
                </script>
              </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dorverbin.github.io/refnerf/index.html">
                    <span class="papertitle">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</span>
                  </a>
                  <br>
                  <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                  <a href="Todd Zickler">Todd Zickler</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
            <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
                  <br>
                  <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
            /
                  <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
            /
                  <a href="https://youtu.be/qrdRH9irAlk">video</a>
                  <p></p>
                  <p>Explicitly modeling reflections in NeRF produces realistic shiny surfaces and accurate surface normals, and lets you edit materials.</p>
                </td>
              </tr>
              
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/mip360_sat.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/mip360_sat.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function mip360_start() {
                    document.getElementById('mip360_image').style.opacity = "1";
                  }

                  function mip360_stop() {
                    document.getElementById('mip360_image').style.opacity = "0";
                  }
                  mip360_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://jonbarron.info/mipnerf360">
                  <span class="papertitle">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="http://jonbarron.info/mipnerf360">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
                /
                <a href="https://youtu.be/zBSH-k9GbV4">video</a>
                <p></p>
                <p>mip-NeRF can be extended to produce realistic results on unbounded scenes.</p>
              </td>
            </tr> 

            <tr onmouseout="rawnerf_stop()" onmouseover="rawnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='rawnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/rawnerf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/rawnerf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function rawnerf_start() {
                    document.getElementById('rawnerf_image').style.opacity = "1";
                  }

                  function rawnerf_stop() {
                    document.getElementById('rawnerf_image').style.opacity = "0";
                  }
                  rawnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://bmild.github.io/rawnerf/index.html">
                  <span class="papertitle">NeRF in the Dark: High Dynamic Range View Synthesis from Noisy Raw Images</span>
                </a>
                <br>
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://bmild.github.io/rawnerf/index.html">project page</a>
          /
                <a href="https://arxiv.org/abs/2111.13679">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=JtBS4KBcKVc">video</a>
                <p></p>
                <p>
                  Properly training NeRF on raw camera data enables HDR view synthesis and bokeh, and outperforms multi-image denoising.</p>
              </td>
            </tr> 
            
    
            <tr onmouseout="regnerf_stop()" onmouseover="regnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='regnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/regnerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/regnerf_before.jpeg' width="160">
                </div>
                <script type="text/javascript">
                  function regnerf_start() {
                    document.getElementById('regnerf_image').style.opacity = "1";
                  }

                  function regnerf_stop() {
                    document.getElementById('regnerf_image').style.opacity = "0";
                  }
                  regnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/regnerf/index.html">
                  <span class="papertitle">RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs</span>
                </a>
                <br>
                <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                <a href="https://msmsajjadi.github.io/">Mehdi S. M. Sajjadi</a>, 
                <a href="http://www.cvlibs.net/">Andreas Geiger</a>,
                <a href="http://www2.informatik.uni-freiburg.de/~radwann/">Noha Radwan</a>
                <br>
          <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://m-niemeyer.github.io/regnerf/index.html">project page</a>
          /
                <a href="https://arxiv.org/abs/2112.00724">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=QyyyvA4-Kwc">video</a>
                <p></p>
                <p>Regularizing unseen views during optimization enables view synthesis from as few as 3 input images.</p>
              </td>
            </tr> 


            <tr onmouseout="blocknerf_stop()" onmouseover="blocknerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='blocknerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/blocknerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/blocknerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function blocknerf_start() {
                    document.getElementById('blocknerf_image').style.opacity = "1";
                  }

                  function blocknerf_stop() {
                    document.getElementById('blocknerf_image').style.opacity = "0";
                  }
                  blocknerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://waymo.com/research/block-nerf/">
                  <span class="papertitle">Block-NeRF: Scalable Large Scene Neural View Synthesis</span>
                </a>
                <br>
                <a href="http://matthewtancik.com/">Matthew Tancik</a>,
                <a href="http://casser.io/">Vincent Casser</a>,
                <a href="https://sites.google.com/site/skywalkeryxc/">Xinchen Yan</a>,
                <a href="https://scholar.google.com/citations?user=5mJUkI4AAAAJ&hl=en">Sabeek Pradhan</a>, <br>
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.henrikkretzschmar.com/">Henrik Kretzschmar</a>
                <br>
          <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://waymo.com/research/block-nerf/">project page</a>
          /
                <a href="https://arxiv.org/abs/2202.05263">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=6lGMCAzBzOQ">video</a>
                <p></p>
                <p>We can do city-scale reconstruction by training multiple NeRFs with millions of images.</p>
              </td>
            </tr>
            
            <tr onmouseout="hnerf_stop()" onmouseover="hnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/hnerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/hnerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function hnerf_start() {
                    document.getElementById('hnerf_image').style.opacity = "1";
                  }

                  function hnerf_stop() {
                    document.getElementById('hnerf_image').style.opacity = "0";
                  }
                  hnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://grail.cs.washington.edu/projects/humannerf/">
                  <span class="papertitle">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</span>
                </a>
                <br>
                <a href="https://homes.cs.washington.edu/~chungyi/">Chung-Yi Weng</a>,
                <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.irakemelmacher.com/">Ira Kemelmacher-Shlizerman </a>
                <br>
                <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://grail.cs.washington.edu/projects/humannerf/">project page</a>
                /
                <a href="https://arxiv.org/abs/2201.04127">arXiv</a>
                /
                <a href="https://youtu.be/GM-RoZEymmw">video</a>
                <p></p>
                <p>Combining NeRF with pose estimation lets you use a monocular video to do free-viewpoint rendering of a human.</p>
              </td>
            </tr>
            
            <tr onmouseout="urf_stop()" onmouseover="urf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='urf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/urf.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/urf.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function urf_start() {
                    document.getElementById('urf_image').style.opacity = "1";
                  }

                  function urf_stop() {
                    document.getElementById('urf_image').style.opacity = "0";
                  }
                  urf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://urban-radiance-fields.github.io/">
                  <span class="papertitle">Urban Radiance Fields</span>
                </a>
                <br>
                <a href="http://www.krematas.com/">Konstantinos Rematas</a>,
                <a href="https://andrewhliu.github.io/">Andrew Liu</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
                <a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a>,
                <a href="https://sites.google.com/corp/view/vittoferrari"> Vittorio Ferrari</a>
                <br>
                <em>CVPR</em>, 2022
                <br>
                <a href="https://urban-radiance-fields.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2111.14643">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=qGlq5DZT6uc">video</a>
                <p></p>
                <p>
                  Incorporating lidar and explicitly modeling the sky lets you reconstruct urban environments.</p>
              </td>
            </tr> 

    
    <tr onmouseout="ddp_stop()" onmouseover="ddp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ddp_image'>
            <img src='images/ddp_after.jpg' width="160"></div>
          <img src='images/ddp_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function ddp_start() {
            document.getElementById('ddp_image').style.opacity = "1";
          }

          function ddp_stop() {
            document.getElementById('ddp_image').style.opacity = "0";
          }
          ddp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2112.03288">
          <span class="papertitle">Dense Depth Priors for Neural Radiance Fields from Sparse Input Views</span>
        </a>
        <br>
        <a href="https://niessnerlab.org/members/barbara_roessle/profile.html">Barbara Roessle</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>, 
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, 
        <a href="https://www.niessnerlab.org/">Matthias Nie√üner</a>
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2112.03288">arXiv</a>
        /
        <a href="https://www.youtube.com/watch?v=zzkvvdcvksc">video</a>
        <p></p>
        <p>
        Dense depth completion techniques applied to freely-available sparse stereo data can improve NeRF reconstructions in low-data regimes.
        </p>
      </td>
    </tr>
    
            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='clipnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/dreamfield_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/dreamfield_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ajayj.com/dreamfields">
                  <span class="papertitle">Zero-Shot Text-Guided Object Generation with Dream Fields</span>
                </a>
                <br>
                <a href="https://www.ajayj.com/">Ajay Jain</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
                <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>
                <br>
          <em>CVPR</em>, 2022
                <br>
                <a href="https://ajayj.com/dreamfields">project page</a>
          /
                <a href="https://arxiv.org/abs/2112.01455">arXiv</a>
          /
                <a href="https://www.youtube.com/watch?v=1Fke6w46tv4">video</a>
                <p></p>
                <p>Supervising the CLIP embeddings of NeRF renderings lets you to generate 3D objects from text prompts.</p>
              </td>
            </tr> 
      
            <tr onmouseout="survey_stop()" onmouseover="survey_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='survey_image'>
                    <img src='images/survey_after.png' width="160"></div>
                  <img src='images/survey_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function survey_start() {
                    document.getElementById('survey_image').style.opacity = "1";
                  }

                  function survey_stop() {
                    document.getElementById('survey_image').style.opacity = "0";
                  }
                  survey_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2111.05849">
                  <span class="papertitle">Advances in Neural Rendering</span>
                </a>
                <br>
                <a href="https://people.mpi-inf.mpg.de/~atewari/">Ayush Tewari</a>, 
                <a href="https://justusthies.github.io/">Justus Thies</a>, 
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, 
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>, 
                <a href="https://people.mpi-inf.mpg.de/~tretschk/">Edgar Tretschk</a>,
                <a href="https://homes.cs.washington.edu/~yifan1/">Yifan Wang</a>,
                <a href="https://christophlassner.de/">Christoph Lassner</a>,
                <a href="https://vsitzmann.github.io/">Vincent Sitzmann</a>,
                <a href="http://ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                <a href="https://stephenlombardi.github.io/">Stephen Lombardi</a>,
                <a href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>,
                <a href="https://www.mpi-inf.mpg.de/departments/visual-computing-and-artificial-intelligence">Christian Theobalt</a>,
                <a href="https://www.niessnerlab.org/">Matthias Niessner</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                <a href="https://zollhoefer.com/">Michael Zollhoefer</a>,
                <a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>
                <br>
				<em>State of the Art Report at EUROGRAPHICS<em>, 2022
                <br>
                <p></p>
                <p>
                A survey of recent progress in neural rendering.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="npil_stop()" onmouseover="npil_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='npil_image'>
                    <img src='images/npil_after.jpg' width="160"></div>
                  <img src='images/npil_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function npil_start() {
                    document.getElementById('npil_image').style.opacity = "1";
                  }

                  function npil_stop() {
                    document.getElementById('npil_image').style.opacity = "0";
                  }
                  npil_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me/publication/2021-neural-pil/">
                  <span class="papertitle">Neural-PIL: Neural Pre-Integrated Lighting for Reflectance Decomposition</span>
                </a>
                <br>

                <a href="https://markboss.me">Mark Boss</a>, 
                <a href="https://varunjampani.github.io">Varun Jampani</a>,
                <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>, <br>
                <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
                <br>
                <em>NeurIPS</em>, 2021
                <br>
                <a href="https://markboss.me/publication/2021-neural-pil/">project page</a> /
                <a href="https://www.youtube.com/watch?v=p5cKaNwVp4M">video</a> /
                <a href="https://arxiv.org/abs/2110.14373">arXiv</a>
                <p></p>
                <p>
                Replacing a costly illumination integral with a simple network query enables more accurate novel view-synthesis and relighting compared to NeRD.
                </p>
              </td>
            </tr>
            
            
            <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='hypernerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/hypernerf_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/hypernerf_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function hypernerf_start() {
                    document.getElementById('hypernerf_image').style.opacity = "1";
                  }

                  function hypernerf_stop() {
                    document.getElementById('hypernerf_image').style.opacity = "0";
                  }
                  hypernerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://hypernerf.github.io/">
                  <span class="papertitle">HyperNeRF: A Higher-Dimensional Representation
for Topologically Varying Neural Radiance Fields</span>
                </a>
                <br>
                <a href="https://keunhong.com">Keunhong Park</a>,
                <a href="https://utkarshsinha.com">Utkarsh Sinha</a>, 
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,
                <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
                <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a>, 
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>
                <br>
                <em>SIGGRAPH Asia</em>, 2021 
                <br>
                <a href="https://hypernerf.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2106.13228">arXiv</a>
                <p></p>
                <p>Applying ideas from level set methods to NeRF lets you represent scenes that deform and change shape.</p>
              </td>
            </tr> 

            <tr onmouseout="nerfactor_stop()" onmouseover="nerfactor_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfactor_image'>
                    <img src='images/nerfactor_after.png' width="160"></div>
                  <img src='images/nerfactor_before.png' width="160">
                </div>
                <script type="text/javascript">
                  function nerfactor_start() {
                    document.getElementById('nerfactor_image').style.opacity = "1";
                  }

                  function nerfactor_stop() {
                    document.getElementById('nerfactor_image').style.opacity = "0";
                  }
                  nerfactor_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">
                <span class="papertitle">NeRFactor: Neural Factorization of Shape and Reflectance<br>
  Under an Unknown Illumination</span>
                </a>
                <br>
                <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://boyangdeng.com/">Boyang Deng</a>,<br>
                <a href="https://www.pauldebevec.com/">Paul Debevec</a>,
                <a href="http://billf.mit.edu/">William T. Freeman</a>,
                <strong>Jonathan T. Barron</strong>
                <br>
                <em>SIGGRAPH Asia</em>, 2021 
                <br>
                <a href="https://people.csail.mit.edu/xiuming/projects/nerfactor/">project page</a>
                /
                <a href="https://arxiv.org/abs/2106.01970">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=UUVSPJlwhPg">video</a>
                <p></p>
                <p>By placing priors on illumination and materials, we can recover NeRF-like models of the intrinsics of a scene from a single multi-image capture.</p>
              </td>
              
            <tr onmouseout="dualfont_stop()" onmouseover="dualfont_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualfont_image'><img src='images/dualfont_after.png'></div>
                  <img src='images/dualfont_before.png'>
                </div>
                <script type="text/javascript">
                  function dualfont_start() {
                    document.getElementById('dualfont_image').style.opacity = "1";
                  }

                  function dualfont_stop() {
                    document.getElementById('dualfont_image').style.opacity = "0";
                  }
                  dualfont_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2109.06627">
                  <span class="papertitle">Scalable Font Reconstruction with Dual Latent Manifolds</span>
                </a>
                <br>
                <a href="http://www.cs.cmu.edu/~asrivats/">Nikita Srivatsan</a>,
                <a href="http://siwu.io/">Si Wu</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://cseweb.ucsd.edu/~tberg/">Taylor Berg-Kirkpatrick</a>
                <br>
                <em>EMNLP</em>, 2021
                <br>
                <p></p>
                <p>VAEs can be used to disentangle a font's style from its content, and to generalize to characters that were never observed during training.</p>
              </td>
            </tr>
            
            <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/mipnerf_ipe_yellow.png' width="160">
                </div>
                <script type="text/javascript">
                  function mipnerf_start() {
                    document.getElementById('mipnerf_image').style.opacity = "1";
                  }

                  function mipnerf_stop() {
                    document.getElementById('mipnerf_image').style.opacity = "0";
                  }
                  mipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://jonbarron.info/mipnerf">
                  <span class="papertitle">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
                <br>
                <a href="http://jonbarron.info/mipnerf">project page</a>
                /
                <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
                /
                <a href="https://youtu.be/EpH175PY1A0">video</a>
                /
                <a href="https://github.com/google/mipnerf">code</a>
                <p></p>
                <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
              </td>
            </tr> 

            <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfbake_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerfbake_15.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerfbake_160.png' width="160">
                </div>
                <script type="text/javascript">
                  function nerfbake_start() {
                    document.getElementById('nerfbake_image').style.opacity = "1";
                  }

                  function nerfbake_stop() {
                    document.getElementById('nerfbake_image').style.opacity = "0";
                  }
                  nerfbake_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://nerf.live">
                <span class="papertitle">Baking Neural Radiance Fields for Real-Time View Synthesis</span>
                </a>
                <br>
                <a href="https://phogzone.com/">Peter Hedman</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="https://www.pauldebevec.com/">Paul Debevec</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="http://nerf.live">project page</a>
                /
                <a href="https://arxiv.org/abs/2103.14645">arXiv</a>
                /
                <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">video</a>
                /
                <a href="https://nerf.live/#demos">demo</a>
                <p></p>
                <p>Baking a trained NeRF into a sparse voxel grid of colors and features lets you render it in real-time in your browser.</p>
              </td>



            <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfie_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerfie_after.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerfie_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerfie_start() {
                    document.getElementById('nerfie_image').style.opacity = "1";
                  }
                  function nerfie_stop() {
                    document.getElementById('nerfie_image').style.opacity = "0";
                  }
                  nerfie_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://nerfies.github.io/">
                  <span class="papertitle">Nerfies: Deformable Neural Radiance Fields</span>
                </a>
                <br>
                
                <a href="https://keunhong.com">Keunhong Park</a>,
                <a href="https://utkarshsinha.com">Utkarsh Sinha</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="http://sofienbouaziz.com">Sofien Bouaziz</a>,
                <a href="https://www.danbgoldman.com">Dan B Goldman</a>,
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a>,
                <a href="http://www.ricardomartinbrualla.com">Ricardo-Martin Brualla</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://nerfies.github.io/">project page</a> /
                <a href="https://arxiv.org/abs/2011.12948">arXiv</a> /
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA">video</a>
                <p></p>
                <p>Building deformation fields into NeRF lets you capture non-rigid subjects, like people.
                </p>
              </td>
            </tr> 


            <tr onmouseout="c5_stop()" onmouseover="c5_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='c5_image'>
                    <img src='images/c5_after.jpg' width="160"></div>
                  <img src='images/c5_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function c5_start() {
                    document.getElementById('c5_image').style.opacity = "1";
                  }

                  function c5_stop() {
                    document.getElementById('c5_image').style.opacity = "0";
                  }
                  c5_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2011.11890">
                  <span class="papertitle">Cross-Camera Convolutional Color Constancy</span>
                </a>
                <br>
                <a href="https://sites.google.com/corp/view/mafifi">Mahmoud Afifi</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.chloelegendre.com/">Chloe LeGendre</a>,
                <a href="https://research.google/people/105312/">Yun-Ta Tsai</a>,
                <a href="https://www.linkedin.com/in/fbleibel/">Francois Bleibel</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <p></p>
                <p>
                  With some extra (unlabeled) test-set images, you can build a hypernetwork that calibrates itself at test time to previously-unseen cameras.
                </p>
              </td>
            </tr> 


            <tr onmouseout="dualdefocus_stop()" onmouseover="dualdefocus_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='dualdefocus_image'>
                    <img src='images/dualdefocus_after.jpg' width="160"></div>
                  <img src='images/dualdefocus_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function dualdefocus_start() {
                    document.getElementById('dualdefocus_image').style.opacity = "1";
                  }

                  function dualdefocus_stop() {
                    document.getElementById('dualdefocus_image').style.opacity = "0";
                  }
                  dualdefocus_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">
                  <span class="papertitle">Defocus Map Estimation and Deblurring from a Single Dual-Pixel Image</span>
                </a>
                <br>
                <a href="https://shumianxin.github.io/">Shumian Xin</a>,
                <a href="http://nealwadhwa.com">Neal Wadhwa</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>,
                <strong>Jonathan T. Barron</strong>, <br>
                <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
                <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,
                <a href="https://www.cs.cmu.edu/~igkioule/">Ioannis Gkioulekas</a>,
                <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>
                <br>
                <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://imaging.cs.cmu.edu/dual_pixels/">project page</a> /
                <a href="https://github.com/cmu-ci-lab/dual_pixel_defocus_estimation_deblurring">code</a>
                <br>
                <p></p>
                <p>
                  Multiplane images can be used to simultaneously deblur dual-pixel images, despite variable defocus due to depth variation in the scene.
                </p>
              </td>
            </tr> 


            <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerd_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerd_160.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerd_160.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerd_start() {
                    document.getElementById('nerd_image').style.opacity = "1";
                  }

                  function nerd_stop() {
                    document.getElementById('nerd_image').style.opacity = "0";
                  }
                  nerd_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://markboss.me/publication/2021-nerd/">
                  <span class="papertitle">NeRD: Neural Reflectance Decomposition from Image Collections</span>
                </a>
                <br>

                <a href="https://markboss.me">Mark Boss</a>, 
                <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/raphael-braun/">Raphael Braun</a>,
                <a href="https://varunjampani.github.io">Varun Jampani</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>,
                <a href="https://uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/lehrstuehle/computergrafik/computer-graphics/staff/prof-dr-ing-hendrik-lensch/">Hendrik P. A. Lensch</a>
                <br>
                <em>ICCV</em>, 2021
                <br>
                <a href="https://markboss.me/publication/2021-nerd/">project page</a> /
                <a href="https://www.youtube.com/watch?v=JL-qMTXw9VU">video</a> /
                <a href="https://github.com/cgtuebingen/NeRD-Neural-Reflectance-Decomposition">code</a> /
                <a href="https://arxiv.org/abs/2012.03918">arXiv</a>
                <p></p>
                <p>
                A NeRF-like model that can decompose (and mesh) objects with non-Lambertian reflectances, complex geometry, and unknown illumination.
                </p>
              </td>
            </tr>





            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
		Templete from Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
		  
        </td>
      </tr>
    </table>
  </body>
</html>
